#version 450
#extension GL_KHR_shader_subgroup_basic: enable
#extension GL_KHR_shader_subgroup_ballot: enable
#extension GL_KHR_shader_subgroup_arithmetic: enable
#extension GL_KHR_shader_subgroup_vote: enable
#extension GL_KHR_shader_subgroup_shuffle_relative : enable
#extension GL_KHR_shader_subgroup_shuffle : enable
#extension  GL_EXT_shader_explicit_arithmetic_types_int64 : enable
#extension GL_EXT_shader_subgroup_extended_types_int64 : enable

#define RADIX_BIN 256
#define RADIX_LOG 8
#define RADIX_BITS 8
#define RADIX_MASK 255 // Mask of digit bins
#define RADIX_PASS 4//(sizeof(uint) * 8 + RADIX_BITS - 1) / RADIX_BITS

#define LANE_COUNT 16 // number of threads in a subgroup
#define LANE_MASK 15
#define LANE_LOG 4

#define LANE gl_LocalInvocationID.x // the idx of thread in the subgroup
#define SUBGROUP_IDX  gl_SubgroupID // the idx of subgroup the thread belongs to might be wrong
#define SUBGROUP_THREAD_IDX gl_GlobalInvocationID.x //(LANE + (SUBGROUP_IDX << LANE_LOG)) // the subgroup relative thread idx                                 


//For the binning
#define MAX_SHEM            4096    //The maximum number of bytes in shared memory
#define BIN_PART_SIZE       3840    //The partition tile size of a BinningPass threadblock
#define BIN_THREADS         256     //The number of threads in a BinningPass threadblock
#define BIN_KEYS_PER_THREAD 15      //The number of keys per thread in BinningPass threadblock previously 15



#define FLAG_NOT_READY      0       //Flag value inidicating neither inclusive sum, or aggregate sum of a partition tile is ready
#define FLAG_AGGREGATE      1       //Flag value indicating aggregate sum of a partition tile is ready
#define FLAG_INCLUSIVE      2       //Flag value indicating inclusive sum of a partition tile is ready
#define FLAG_MASK           3       //Mask used to retrieve flag values



layout(set = 1, binding = 0)  buffer BSortBuffer {
    uint b_sort[];
};

layout(set = 1, binding = 2)  buffer BAltBuffer {
    uint b_alt[];
};

layout(set = 1, binding = 1)  buffer BGlobalHist {
    uint b_globalHist[1024];
};

layout(set = 1, binding = 3) coherent buffer BIndex {
    uint b_index[4];
};

// todo: change it to uint
layout(set = 1, binding = 4) coherent buffer BPassHist {
    uint b_passHist[];
};

layout(push_constant) uniform Constant{
    uint pass_num;
    uint radix_shift;
    uint n;
    uint e_workgroups;
};


layout(local_size_x = 256) in;

shared uint s_subgroupHistograms[MAX_SHEM];


uint descending_index(uint index){
    return n - index - 1;
}

void write_key(uint index, uint group_shared_index){
    pass_num % 2 == 0 ? b_alt[index] = s_subgroupHistograms[group_shared_index] : b_sort[index] = s_subgroupHistograms[group_shared_index];
}

uint pass_hist_offset(uint partition_index, const uint logical_blocks){
    return (((radix_shift >> 3) * logical_blocks) + partition_index) << RADIX_LOG;
}

uint InclusiveWarpScanCircularShift(uint val){
    for (uint i = 1; i <= (LANE_COUNT >> 1); i <<= 1){
        const uint t = subgroupShuffleUp(val, i);
        if (gl_SubgroupInvocationID >= i)
            val += t;
    }
    return subgroupShuffle(val, (gl_SubgroupInvocationID + LANE_MASK) & LANE_MASK);
}


uint ActiveExclusiveWarpScan(uint val) {
    uint result = 0;
    for (uint i = 1; i <= (LANE_COUNT >> 1); i <<= 1){
        uint t = subgroupShuffleUp(val, i);
        if (gl_SubgroupInvocationID >= i)
            val += t;
    }
    const uint t = subgroupShuffleUp(val, 1);
    return (gl_SubgroupInvocationID > 0) ? t : 0;
}

uint64_t getLaneMaskLt() {
    uint laneId = gl_SubgroupInvocationID;
    uint64_t mask = 0;

    // Generate the mask dynamically based on the current laneId
    // For each bit position less than laneId, set the corresponding bit in mask
    for (uint i = 0; i < laneId; ++i) {
        mask |= (1u << i);
    }

    return mask;
}


uint subgroup_hist_size_ge16(){
    return BIN_THREADS / LANE_COUNT * RADIX_BIN;
}

void clear_shared_memory(uint threadidx){
    const uint hist_end = subgroup_hist_size_ge16();
    for(uint i = threadidx; i < hist_end; i += BIN_THREADS){
        s_subgroupHistograms[i] = 0;
    }
}


uint sub_part_size_ge16(){
    return BIN_KEYS_PER_THREAD * LANE_COUNT;
}

uint shared_offset_ge16(uint threadidx){
    return gl_SubgroupInvocationID + gl_SubgroupID * sub_part_size_ge16();
}

uint device_offset_ge16(uint threadidx, uint partition_index){
    return shared_offset_ge16(threadidx) + partition_index * BIN_PART_SIZE;
}


void load_workgroup_reductions(uint threadidx, uint partition_index, uint exclusive_reduction){
    if (threadidx < RADIX_BIN){
        s_subgroupHistograms[threadidx + BIN_PART_SIZE] = b_globalHist[threadidx + (radix_shift << 5)] +
            b_passHist[threadidx * e_workgroups + partition_index] - exclusive_reduction;
    }
}

void load_keys_ge16(uint threadidx, uint partition_index, inout uint keys[BIN_KEYS_PER_THREAD]){
    for(uint i = 0, t = device_offset_ge16(threadidx, partition_index); i < BIN_KEYS_PER_THREAD; ++i, t += LANE_COUNT){
        pass_num % 2 == 0 ? keys[i] = b_sort[t] : keys[i] = b_alt[t];
    }
}


void load_keys_partial_ge16(uint threadidx, uint partition_index,  inout uint keys[BIN_KEYS_PER_THREAD]){
    for(uint i = 0, t = device_offset_ge16(threadidx, partition_index); i < BIN_KEYS_PER_THREAD; ++i, t += LANE_COUNT){
        if (t < n){
            pass_num % 2 == 0 ? keys[i] = b_sort[t] : keys[i] = b_alt[t];
        }else{
            keys[i] = 0xffffffff;
        }
    }
}

uvec4 subgroup_flag_ge16(){
    uvec4 subgroup_flags;
    uint flag_val = (LANE_COUNT & 31) > 0 ? (1 << LANE_COUNT) - 1 : 0xffffffff;
    subgroup_flags = uvec4(flag_val, flag_val, flag_val, flag_val);
    return subgroup_flags;
}

void subgroup_multi_split_ge16(uint key, uint subgroup_parts, inout uvec4 subgroup_flags){
    for(uint k = 0; k < RADIX_LOG; ++k){
        const bool t = (key >> (k + radix_shift) & 1) > 0;
        const uvec4 ballot = subgroupBallot(t);
        for (uint subgroup_part = 0; subgroup_part < subgroup_parts; ++subgroup_part)
            subgroup_flags[subgroup_part] &= (t ? 0 : 0xffffffff) ^ ballot[subgroup_part];

    }
}

uint extract_digit(uint key){
    return key >> radix_shift & RADIX_MASK;
}

uint find_lowest_rank_peer(uvec4 subgroup_flags, uint subgroup_parts){
    uint lowest_rank_peer = 0;
    for(uint subgroup_part = 0; subgroup_part < subgroup_parts; ++subgroup_part){
        uint lowest_bit = findLSB(subgroup_flags[subgroup_part]);
        // todo: maybewrong
        // 0xffffffff is two's complement of -1
        if (lowest_bit == 0xffffffff)
            lowest_rank_peer += 32;
        else
            return lowest_rank_peer + lowest_bit;
    }
    return 0;
}

void count_peer_bits(inout uint peer_bits, inout uint total_bits, uvec4 subgroup_flags, uint subgroup_parts){
    for (uint subgroup_part = 0; subgroup_part < subgroup_parts; ++subgroup_part){
        if (gl_SubgroupInvocationID >= subgroup_part * 32){
            const uint lt_mask = gl_SubgroupInvocationID >= (subgroup_part + 1) * 32 ? 0xffffffff : (1 << (gl_SubgroupInvocationID & 31)) - 1;
            peer_bits += bitCount(subgroup_flags[subgroup_part] & lt_mask);
        }
        total_bits += bitCount(subgroup_flags[subgroup_part]);
    }
}

void rank_keys_ge16(uint threadidx, uint keys[BIN_KEYS_PER_THREAD], inout uint offsets[BIN_KEYS_PER_THREAD]){
    const uint subgroup_parts = (LANE_COUNT + 31)/ 32;
    for(uint i = 0; i < BIN_KEYS_PER_THREAD; ++i){
        uvec4 subgroup_flags = subgroup_flag_ge16();

        subgroup_multi_split_ge16(keys[i], subgroup_parts, subgroup_flags);

        const uint index = extract_digit(keys[i]) + (gl_SubgroupID * RADIX_BIN);
        const uint lowest_rank_peer = find_lowest_rank_peer(subgroup_flags, subgroup_parts);

        uint peer_bits = 0;
        uint total_bits = 0;
        count_peer_bits(peer_bits, total_bits, subgroup_flags, subgroup_parts);
        uint pre_val;
        if (peer_bits == 0)
            pre_val = atomicAdd(s_subgroupHistograms[index], total_bits);
        
        offsets[i] = subgroupShuffle(pre_val, lowest_rank_peer) + peer_bits;
        
    }
}

uint subgroup_hist_inclusive_scan_circle_shift_ge16(uint threadidx){
    uint reduction = s_subgroupHistograms[threadidx];
    for(uint i = threadidx + RADIX_BIN; i < subgroup_hist_size_ge16(); i += RADIX_BIN){
        reduction += s_subgroupHistograms[i];
        s_subgroupHistograms[i] = reduction- s_subgroupHistograms[i];
    }
    return reduction;
}

void subgroup_hist_exclusive_scan_ge16(uint threadidx, uint reduction){
    if (threadidx < RADIX_BIN){
        const uint lane_mask = LANE_COUNT - 1;
        s_subgroupHistograms[((gl_SubgroupInvocationID + 1) & lane_mask) + (threadidx & ~lane_mask)] = reduction;
    }
    groupMemoryBarrier();
    barrier();

    if (threadidx < RADIX_BIN / LANE_COUNT){
        s_subgroupHistograms[threadidx * LANE_COUNT] = subgroupExclusiveAdd(s_subgroupHistograms[threadidx * LANE_COUNT]);
    }

    groupMemoryBarrier();
    barrier();

    if (threadidx < RADIX_BIN && gl_SubgroupInvocationID > 0){
        s_subgroupHistograms[threadidx] += subgroupBroadcast(s_subgroupHistograms[threadidx - 1], 1);
    }

}

void device_broadcast_reduction_ge16(uint threadidx, uint partition_index, uint reduction, const uint logical_blocks){
    if (partition_index < logical_blocks - 1){
        atomicAdd(b_passHist[threadidx + pass_hist_offset(partition_index + 1, logical_blocks)], FLAG_AGGREGATE | reduction << 2);
        
    }
}


void update_offset_ge16(uint threadidx, inout uint offsets[BIN_KEYS_PER_THREAD], uint keys[BIN_KEYS_PER_THREAD]){
    if (threadidx >= LANE_COUNT){
        const uint t = gl_SubgroupID * RADIX_BIN;

        for (uint i = 0; i < BIN_KEYS_PER_THREAD; ++i){
            const uint t2 = extract_digit(keys[i]);
            offsets[i] += s_subgroupHistograms[t2 + t] + s_subgroupHistograms[t2];
        }
    }
    else{
        for (uint i = 0; i < BIN_KEYS_PER_THREAD; ++i){
            offsets[i] += s_subgroupHistograms[extract_digit(keys[i])];
        }
    }
}

void scatter_keys_shared(uint offsets[BIN_KEYS_PER_THREAD], uint keys[BIN_KEYS_PER_THREAD]){
    for (uint i = 0; i < BIN_KEYS_PER_THREAD; ++i){
        s_subgroupHistograms[offsets[i]] = keys[i];
    }
}

void lookback(uint threadidx, uint partition_index, uint exclusive_reduction, const uint logical_blocks){
    if (threadidx < RADIX_BIN){
        uint lookback_reduction = 0;
        for (uint k = partition_index; k >= 0;){
            const uint flag_payload = b_passHist[threadidx + pass_hist_offset(k, logical_blocks)];
            if ((flag_payload & FLAG_MASK) == FLAG_INCLUSIVE){
                lookback_reduction += flag_payload >> 2;
                if (partition_index < logical_blocks - 1){
                    atomicAdd(b_passHist[threadidx + pass_hist_offset(partition_index + 1, logical_blocks)], 1 | lookback_reduction << 2);
                }
                s_subgroupHistograms[threadidx + BIN_PART_SIZE] = lookback_reduction - exclusive_reduction;
                break;
            }

            if ((flag_payload & FLAG_MASK) == FLAG_AGGREGATE){
                lookback_reduction += flag_payload >> 2;
                k--;
            }
        }
    }
}

void scatter_device_ascending(uint threadidx){
    for (uint i = threadidx; i < BIN_PART_SIZE; i += BIN_THREADS){
        write_key(s_subgroupHistograms[extract_digit(s_subgroupHistograms[i]) + BIN_PART_SIZE] + i, i);
    }
}

void scatter_device(uint threadidx){
    if (radix_shift == 24){
        for (uint i = threadidx; i < BIN_PART_SIZE; i += BIN_THREADS){
            write_key(descending_index(s_subgroupHistograms[extract_digit(s_subgroupHistograms[i]) + BIN_PART_SIZE] + i), i);
        }
    }else{
        scatter_device_ascending(threadidx);
    }
}

void scatter_device_partial_asceding(uint threadidx, uint final_part_size){
    for (uint i = threadidx; i < BIN_PART_SIZE; i += BIN_THREADS){
        if (i < final_part_size){
            write_key(s_subgroupHistograms[extract_digit(s_subgroupHistograms[i]) + BIN_PART_SIZE] + i, i);
        }
    }
}
void scatter_device_partial(uint threadidx, uint partition_index){
    const uint final_part_size = n - partition_index * BIN_PART_SIZE;

    scatter_device_partial_asceding(threadidx, final_part_size);
    
}

void k_DigitBinning(){
    uint partition_index = gl_WorkGroupID.x;
    uint keys[BIN_KEYS_PER_THREAD];
    uint offsets[BIN_KEYS_PER_THREAD];


    clear_shared_memory(gl_LocalInvocationID.x);
    groupMemoryBarrier();
    barrier();
    
    
    if (partition_index < e_workgroups - 1){
            load_keys_ge16(gl_LocalInvocationID.x, partition_index, keys);
    }

    if (partition_index == e_workgroups - 1){
        load_keys_partial_ge16(gl_LocalInvocationID.x, partition_index, keys);
    }

    uint exclusive_reduction;
    groupMemoryBarrier();
    barrier();
    rank_keys_ge16(gl_LocalInvocationID.x, keys, offsets);
    groupMemoryBarrier();
    barrier();

    uint reduction;
    if (gl_LocalInvocationID.x < RADIX_BIN){
        reduction = subgroup_hist_inclusive_scan_circle_shift_ge16(gl_LocalInvocationID.x);
        reduction += subgroupExclusiveAdd(reduction);
    }
    groupMemoryBarrier();
    barrier();

    subgroup_hist_exclusive_scan_ge16(gl_LocalInvocationID.x, reduction);
    groupMemoryBarrier();
    barrier();

    update_offset_ge16(gl_LocalInvocationID.x, offsets, keys);
    if (gl_LocalInvocationID.x < RADIX_BIN)
        exclusive_reduction = s_subgroupHistograms[gl_LocalInvocationID.x];
    groupMemoryBarrier();
    barrier();

    scatter_keys_shared(offsets, keys);
    load_workgroup_reductions(gl_LocalInvocationID.x, partition_index, exclusive_reduction);
    groupMemoryBarrier();
    barrier();

    if (partition_index < e_workgroups - 1){
        //scatter_device(gl_LocalInvocationID.x);
        scatter_device_ascending(gl_LocalInvocationID.x);
    }
    
    if (partition_index == e_workgroups - 1){
        scatter_device_partial(gl_LocalInvocationID.x, partition_index);

    }
}


void main(){
    k_DigitBinning();
}